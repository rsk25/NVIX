{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from bertviz import head_view\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.swan import SWANPhase1Only\n",
    "from model.base import chkpt\n",
    "from test_model import load_config, run_model_for_attention\n",
    "from common.dataset import Dataset\n",
    "from learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = Path('.')\n",
    "data_path = main_path / 'resource'\n",
    "chpt_path = main_path / 'runs_copy' / 'best_SWAN_P1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(chpt_path / 'tokenizer.pt')\n",
    "checkpoint = torch.load(chpt_path / 'SWANPhase1Only.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at google/electra-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraModel were not initialized from the model checkpoint at google/electra-base-discriminator and are newly initialized: ['electra.encoder.layer.2.crossattention.self.query.bias', 'electra.encoder.layer.5.crossattention.self.query.weight', 'electra.encoder.layer.3.crossattention.output.dense.bias', 'electra.encoder.layer.6.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.5.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.4.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.3.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.7.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.2.crossattention.self.key.bias', 'electra.encoder.layer.7.crossattention.output.dense.weight', 'electra.encoder.layer.0.crossattention.self.value.bias', 'electra.encoder.layer.8.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.1.crossattention.self.value.weight', 'electra.encoder.layer.8.crossattention.self.key.bias', 'electra.encoder.layer.4.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.0.crossattention.self.value.weight', 'electra.encoder.layer.0.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.3.crossattention.self.value.weight', 'electra.encoder.layer.0.crossattention.self.key.weight', 'electra.encoder.layer.10.crossattention.self.value.weight', 'electra.encoder.layer.6.crossattention.self.query.bias', 'electra.encoder.layer.10.crossattention.output.dense.weight', 'electra.encoder.layer.11.crossattention.self.query.bias', 'electra.encoder.layer.3.crossattention.self.key.bias', 'electra.encoder.layer.1.crossattention.self.value.bias', 'electra.encoder.layer.4.crossattention.output.dense.weight', 'electra.encoder.layer.0.crossattention.output.dense.bias', 'electra.encoder.layer.3.crossattention.output.dense.weight', 'electra.encoder.layer.4.crossattention.self.value.bias', 'electra.encoder.layer.5.crossattention.self.key.bias', 'electra.encoder.layer.2.crossattention.self.query.weight', 'electra.encoder.layer.5.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.9.crossattention.self.value.weight', 'electra.encoder.layer.8.crossattention.self.key.weight', 'electra.encoder.layer.11.crossattention.self.value.bias', 'electra.encoder.layer.11.crossattention.self.key.weight', 'electra.encoder.layer.7.crossattention.self.query.bias', 'electra.encoder.layer.5.crossattention.self.query.bias', 'electra.encoder.layer.3.crossattention.self.query.bias', 'electra.encoder.layer.7.crossattention.self.value.weight', 'electra.encoder.layer.9.crossattention.self.key.bias', 'electra.encoder.layer.2.crossattention.self.value.bias', 'electra.encoder.layer.9.crossattention.output.dense.weight', 'electra.encoder.layer.8.crossattention.self.query.bias', 'electra.encoder.layer.8.crossattention.self.value.bias', 'electra.encoder.layer.6.crossattention.self.key.weight', 'electra.encoder.layer.9.crossattention.self.query.weight', 'electra.encoder.layer.9.crossattention.self.key.weight', 'electra.encoder.layer.1.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.7.crossattention.output.dense.bias', 'electra.encoder.layer.6.crossattention.output.dense.bias', 'electra.encoder.layer.1.crossattention.output.dense.bias', 'electra.encoder.layer.8.crossattention.self.value.weight', 'electra.encoder.layer.11.crossattention.self.value.weight', 'electra.encoder.layer.10.crossattention.self.query.bias', 'electra.encoder.layer.8.crossattention.self.query.weight', 'electra.encoder.layer.1.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.4.crossattention.self.query.bias', 'electra.encoder.layer.7.crossattention.self.key.bias', 'electra.encoder.layer.10.crossattention.self.value.bias', 'electra.encoder.layer.11.crossattention.self.key.bias', 'electra.encoder.layer.11.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.10.crossattention.self.query.weight', 'electra.encoder.layer.6.crossattention.self.query.weight', 'electra.encoder.layer.4.crossattention.self.key.weight', 'electra.encoder.layer.5.crossattention.self.key.weight', 'electra.encoder.layer.3.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.1.crossattention.self.key.weight', 'electra.encoder.layer.4.crossattention.self.value.weight', 'electra.encoder.layer.2.crossattention.self.value.weight', 'electra.encoder.layer.9.crossattention.self.query.bias', 'electra.encoder.layer.5.crossattention.self.value.weight', 'electra.encoder.layer.8.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.1.crossattention.self.key.bias', 'electra.encoder.layer.3.crossattention.self.key.weight', 'electra.encoder.layer.7.crossattention.self.query.weight', 'electra.encoder.layer.4.crossattention.self.key.bias', 'electra.encoder.layer.9.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.8.crossattention.output.dense.weight', 'electra.encoder.layer.0.crossattention.self.key.bias', 'electra.encoder.layer.11.crossattention.self.query.weight', 'electra.encoder.layer.11.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.1.crossattention.output.dense.weight', 'electra.encoder.layer.0.crossattention.self.query.weight', 'electra.encoder.layer.6.crossattention.self.key.bias', 'electra.encoder.layer.6.crossattention.self.value.bias', 'electra.encoder.layer.7.crossattention.self.value.bias', 'electra.encoder.layer.9.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.1.crossattention.self.query.bias', 'electra.encoder.layer.0.crossattention.output.dense.weight', 'electra.encoder.layer.10.crossattention.output.dense.bias', 'electra.encoder.layer.2.crossattention.output.dense.weight', 'electra.encoder.layer.9.crossattention.self.value.bias', 'electra.encoder.layer.11.crossattention.output.dense.weight', 'electra.encoder.layer.10.crossattention.self.key.weight', 'electra.encoder.layer.2.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.7.crossattention.self.key.weight', 'electra.encoder.layer.1.crossattention.self.query.weight', 'electra.encoder.layer.2.crossattention.self.key.weight', 'electra.encoder.layer.4.crossattention.self.query.weight', 'electra.encoder.layer.7.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.11.crossattention.output.dense.bias', 'electra.encoder.layer.10.crossattention.output.LayerNorm.weight', 'electra.encoder.layer.10.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.6.crossattention.output.dense.weight', 'electra.encoder.layer.9.crossattention.output.dense.bias', 'electra.encoder.layer.6.crossattention.self.value.weight', 'electra.encoder.layer.3.crossattention.self.value.bias', 'electra.encoder.layer.5.crossattention.output.dense.bias', 'electra.encoder.layer.6.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.0.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.4.crossattention.output.dense.bias', 'electra.encoder.layer.2.crossattention.output.LayerNorm.bias', 'electra.encoder.layer.8.crossattention.output.dense.bias', 'electra.encoder.layer.5.crossattention.self.value.bias', 'electra.encoder.layer.3.crossattention.self.query.weight', 'electra.encoder.layer.2.crossattention.output.dense.bias', 'electra.encoder.layer.5.crossattention.output.dense.weight', 'electra.encoder.layer.0.crossattention.self.query.bias', 'electra.encoder.layer.10.crossattention.self.key.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SWANPhase1Only(\n",
       "  (encoder): TextEncoder(\n",
       "    (model): ElectraModel(\n",
       "      (embeddings): ElectraEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): ElectraEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): ElectraLayer(\n",
       "            (attention): ElectraAttention(\n",
       "              (self): ElectraSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): ElectraSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ElectraIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            )\n",
       "            (output): ElectraOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (equation): EquationDecoder(\n",
       "    (operator_word_embedding): Embedding(9, 768)\n",
       "    (operator_pos_embedding): PositionalEncoding()\n",
       "    (operand_source_embedding): Embedding(3, 768)\n",
       "    (constant_word_embedding): Embedding(23, 768)\n",
       "    (operator_norm): MaskedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (operand_norm): MaskedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (embed_to_hidden): Linear(in_features=2304, out_features=768, bias=True)\n",
       "    (shared_decoder_layer): TransformerLayer(\n",
       "      (attn): MultiheadAttention(\n",
       "        (attn): MultiheadAttentionWeights(\n",
       "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (mem): MultiheadAttention(\n",
       "        (attn): MultiheadAttentionWeights(\n",
       "          (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (linear_v): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (lin_expand): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      (lin_collapse): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      (gelu): GELU()\n",
       "      (norm_attn): MaskedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm_mem): MaskedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (norm_out): MaskedLayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (operator): Linear(in_features=768, out_features=9, bias=True)\n",
       "  (operands): ModuleList(\n",
       "    (0): ModuleDict(\n",
       "      (0_attn): MultiheadAttentionWeights(\n",
       "        (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1_mean): Squeeze(dim=-1)\n",
       "    )\n",
       "    (1): ModuleDict(\n",
       "      (0_attn): MultiheadAttentionWeights(\n",
       "        (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1_mean): Squeeze(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (explanation): ExplanationDecoder(\n",
       "    (embeddings): ElectraEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): ElectraEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): ElectraLayer(\n",
       "          (attention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (crossattention): ElectraAttention(\n",
       "            (self): ElectraSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): ElectraSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ElectraIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): ElectraOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (explanation_pghead): PointerGeneratorHead(\n",
       "    (encoder_attention): MultiheadAttentionWeights(\n",
       "      (linear_q): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (linear_k): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (generation_dist): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    (generation_prob_linear): Linear(in_features=2304, out_features=1, bias=True)\n",
       "    (log_sigmoid): LogSigmoid()\n",
       "  )\n",
       "  (var_count_expand): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (var_count_predict): Linear(in_features=3072, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = load_config(chpt_path)\n",
    "nvix = SWANPhase1Only.create_or_load(path=str(chpt_path), **config)\n",
    "nvix.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = nvix.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['explanation._prefix_number', 'explanation.embeddings.position_ids', 'explanation.embeddings.word_embeddings.weight', 'explanation.embeddings.position_embeddings.weight', 'explanation.embeddings.token_type_embeddings.weight', 'explanation.embeddings.LayerNorm.weight', 'explanation.embeddings.LayerNorm.bias', 'explanation.encoder.layer.0.attention.self.query.weight', 'explanation.encoder.layer.0.attention.self.query.bias', 'explanation.encoder.layer.0.attention.self.key.weight', 'explanation.encoder.layer.0.attention.self.key.bias', 'explanation.encoder.layer.0.attention.self.value.weight', 'explanation.encoder.layer.0.attention.self.value.bias', 'explanation.encoder.layer.0.attention.output.dense.weight', 'explanation.encoder.layer.0.attention.output.dense.bias', 'explanation.encoder.layer.0.attention.output.LayerNorm.weight', 'explanation.encoder.layer.0.attention.output.LayerNorm.bias', 'explanation.encoder.layer.0.crossattention.self.query.weight', 'explanation.encoder.layer.0.crossattention.self.query.bias', 'explanation.encoder.layer.0.crossattention.self.key.weight', 'explanation.encoder.layer.0.crossattention.self.key.bias', 'explanation.encoder.layer.0.crossattention.self.value.weight', 'explanation.encoder.layer.0.crossattention.self.value.bias', 'explanation.encoder.layer.0.crossattention.output.dense.weight', 'explanation.encoder.layer.0.crossattention.output.dense.bias', 'explanation.encoder.layer.0.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.0.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.0.intermediate.dense.weight', 'explanation.encoder.layer.0.intermediate.dense.bias', 'explanation.encoder.layer.0.output.dense.weight', 'explanation.encoder.layer.0.output.dense.bias', 'explanation.encoder.layer.0.output.LayerNorm.weight', 'explanation.encoder.layer.0.output.LayerNorm.bias', 'explanation.encoder.layer.1.attention.self.query.weight', 'explanation.encoder.layer.1.attention.self.query.bias', 'explanation.encoder.layer.1.attention.self.key.weight', 'explanation.encoder.layer.1.attention.self.key.bias', 'explanation.encoder.layer.1.attention.self.value.weight', 'explanation.encoder.layer.1.attention.self.value.bias', 'explanation.encoder.layer.1.attention.output.dense.weight', 'explanation.encoder.layer.1.attention.output.dense.bias', 'explanation.encoder.layer.1.attention.output.LayerNorm.weight', 'explanation.encoder.layer.1.attention.output.LayerNorm.bias', 'explanation.encoder.layer.1.crossattention.self.query.weight', 'explanation.encoder.layer.1.crossattention.self.query.bias', 'explanation.encoder.layer.1.crossattention.self.key.weight', 'explanation.encoder.layer.1.crossattention.self.key.bias', 'explanation.encoder.layer.1.crossattention.self.value.weight', 'explanation.encoder.layer.1.crossattention.self.value.bias', 'explanation.encoder.layer.1.crossattention.output.dense.weight', 'explanation.encoder.layer.1.crossattention.output.dense.bias', 'explanation.encoder.layer.1.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.1.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.1.intermediate.dense.weight', 'explanation.encoder.layer.1.intermediate.dense.bias', 'explanation.encoder.layer.1.output.dense.weight', 'explanation.encoder.layer.1.output.dense.bias', 'explanation.encoder.layer.1.output.LayerNorm.weight', 'explanation.encoder.layer.1.output.LayerNorm.bias', 'explanation.encoder.layer.2.attention.self.query.weight', 'explanation.encoder.layer.2.attention.self.query.bias', 'explanation.encoder.layer.2.attention.self.key.weight', 'explanation.encoder.layer.2.attention.self.key.bias', 'explanation.encoder.layer.2.attention.self.value.weight', 'explanation.encoder.layer.2.attention.self.value.bias', 'explanation.encoder.layer.2.attention.output.dense.weight', 'explanation.encoder.layer.2.attention.output.dense.bias', 'explanation.encoder.layer.2.attention.output.LayerNorm.weight', 'explanation.encoder.layer.2.attention.output.LayerNorm.bias', 'explanation.encoder.layer.2.crossattention.self.query.weight', 'explanation.encoder.layer.2.crossattention.self.query.bias', 'explanation.encoder.layer.2.crossattention.self.key.weight', 'explanation.encoder.layer.2.crossattention.self.key.bias', 'explanation.encoder.layer.2.crossattention.self.value.weight', 'explanation.encoder.layer.2.crossattention.self.value.bias', 'explanation.encoder.layer.2.crossattention.output.dense.weight', 'explanation.encoder.layer.2.crossattention.output.dense.bias', 'explanation.encoder.layer.2.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.2.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.2.intermediate.dense.weight', 'explanation.encoder.layer.2.intermediate.dense.bias', 'explanation.encoder.layer.2.output.dense.weight', 'explanation.encoder.layer.2.output.dense.bias', 'explanation.encoder.layer.2.output.LayerNorm.weight', 'explanation.encoder.layer.2.output.LayerNorm.bias', 'explanation.encoder.layer.3.attention.self.query.weight', 'explanation.encoder.layer.3.attention.self.query.bias', 'explanation.encoder.layer.3.attention.self.key.weight', 'explanation.encoder.layer.3.attention.self.key.bias', 'explanation.encoder.layer.3.attention.self.value.weight', 'explanation.encoder.layer.3.attention.self.value.bias', 'explanation.encoder.layer.3.attention.output.dense.weight', 'explanation.encoder.layer.3.attention.output.dense.bias', 'explanation.encoder.layer.3.attention.output.LayerNorm.weight', 'explanation.encoder.layer.3.attention.output.LayerNorm.bias', 'explanation.encoder.layer.3.crossattention.self.query.weight', 'explanation.encoder.layer.3.crossattention.self.query.bias', 'explanation.encoder.layer.3.crossattention.self.key.weight', 'explanation.encoder.layer.3.crossattention.self.key.bias', 'explanation.encoder.layer.3.crossattention.self.value.weight', 'explanation.encoder.layer.3.crossattention.self.value.bias', 'explanation.encoder.layer.3.crossattention.output.dense.weight', 'explanation.encoder.layer.3.crossattention.output.dense.bias', 'explanation.encoder.layer.3.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.3.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.3.intermediate.dense.weight', 'explanation.encoder.layer.3.intermediate.dense.bias', 'explanation.encoder.layer.3.output.dense.weight', 'explanation.encoder.layer.3.output.dense.bias', 'explanation.encoder.layer.3.output.LayerNorm.weight', 'explanation.encoder.layer.3.output.LayerNorm.bias', 'explanation.encoder.layer.4.attention.self.query.weight', 'explanation.encoder.layer.4.attention.self.query.bias', 'explanation.encoder.layer.4.attention.self.key.weight', 'explanation.encoder.layer.4.attention.self.key.bias', 'explanation.encoder.layer.4.attention.self.value.weight', 'explanation.encoder.layer.4.attention.self.value.bias', 'explanation.encoder.layer.4.attention.output.dense.weight', 'explanation.encoder.layer.4.attention.output.dense.bias', 'explanation.encoder.layer.4.attention.output.LayerNorm.weight', 'explanation.encoder.layer.4.attention.output.LayerNorm.bias', 'explanation.encoder.layer.4.crossattention.self.query.weight', 'explanation.encoder.layer.4.crossattention.self.query.bias', 'explanation.encoder.layer.4.crossattention.self.key.weight', 'explanation.encoder.layer.4.crossattention.self.key.bias', 'explanation.encoder.layer.4.crossattention.self.value.weight', 'explanation.encoder.layer.4.crossattention.self.value.bias', 'explanation.encoder.layer.4.crossattention.output.dense.weight', 'explanation.encoder.layer.4.crossattention.output.dense.bias', 'explanation.encoder.layer.4.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.4.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.4.intermediate.dense.weight', 'explanation.encoder.layer.4.intermediate.dense.bias', 'explanation.encoder.layer.4.output.dense.weight', 'explanation.encoder.layer.4.output.dense.bias', 'explanation.encoder.layer.4.output.LayerNorm.weight', 'explanation.encoder.layer.4.output.LayerNorm.bias', 'explanation.encoder.layer.5.attention.self.query.weight', 'explanation.encoder.layer.5.attention.self.query.bias', 'explanation.encoder.layer.5.attention.self.key.weight', 'explanation.encoder.layer.5.attention.self.key.bias', 'explanation.encoder.layer.5.attention.self.value.weight', 'explanation.encoder.layer.5.attention.self.value.bias', 'explanation.encoder.layer.5.attention.output.dense.weight', 'explanation.encoder.layer.5.attention.output.dense.bias', 'explanation.encoder.layer.5.attention.output.LayerNorm.weight', 'explanation.encoder.layer.5.attention.output.LayerNorm.bias', 'explanation.encoder.layer.5.crossattention.self.query.weight', 'explanation.encoder.layer.5.crossattention.self.query.bias', 'explanation.encoder.layer.5.crossattention.self.key.weight', 'explanation.encoder.layer.5.crossattention.self.key.bias', 'explanation.encoder.layer.5.crossattention.self.value.weight', 'explanation.encoder.layer.5.crossattention.self.value.bias', 'explanation.encoder.layer.5.crossattention.output.dense.weight', 'explanation.encoder.layer.5.crossattention.output.dense.bias', 'explanation.encoder.layer.5.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.5.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.5.intermediate.dense.weight', 'explanation.encoder.layer.5.intermediate.dense.bias', 'explanation.encoder.layer.5.output.dense.weight', 'explanation.encoder.layer.5.output.dense.bias', 'explanation.encoder.layer.5.output.LayerNorm.weight', 'explanation.encoder.layer.5.output.LayerNorm.bias', 'explanation.encoder.layer.6.attention.self.query.weight', 'explanation.encoder.layer.6.attention.self.query.bias', 'explanation.encoder.layer.6.attention.self.key.weight', 'explanation.encoder.layer.6.attention.self.key.bias', 'explanation.encoder.layer.6.attention.self.value.weight', 'explanation.encoder.layer.6.attention.self.value.bias', 'explanation.encoder.layer.6.attention.output.dense.weight', 'explanation.encoder.layer.6.attention.output.dense.bias', 'explanation.encoder.layer.6.attention.output.LayerNorm.weight', 'explanation.encoder.layer.6.attention.output.LayerNorm.bias', 'explanation.encoder.layer.6.crossattention.self.query.weight', 'explanation.encoder.layer.6.crossattention.self.query.bias', 'explanation.encoder.layer.6.crossattention.self.key.weight', 'explanation.encoder.layer.6.crossattention.self.key.bias', 'explanation.encoder.layer.6.crossattention.self.value.weight', 'explanation.encoder.layer.6.crossattention.self.value.bias', 'explanation.encoder.layer.6.crossattention.output.dense.weight', 'explanation.encoder.layer.6.crossattention.output.dense.bias', 'explanation.encoder.layer.6.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.6.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.6.intermediate.dense.weight', 'explanation.encoder.layer.6.intermediate.dense.bias', 'explanation.encoder.layer.6.output.dense.weight', 'explanation.encoder.layer.6.output.dense.bias', 'explanation.encoder.layer.6.output.LayerNorm.weight', 'explanation.encoder.layer.6.output.LayerNorm.bias', 'explanation.encoder.layer.7.attention.self.query.weight', 'explanation.encoder.layer.7.attention.self.query.bias', 'explanation.encoder.layer.7.attention.self.key.weight', 'explanation.encoder.layer.7.attention.self.key.bias', 'explanation.encoder.layer.7.attention.self.value.weight', 'explanation.encoder.layer.7.attention.self.value.bias', 'explanation.encoder.layer.7.attention.output.dense.weight', 'explanation.encoder.layer.7.attention.output.dense.bias', 'explanation.encoder.layer.7.attention.output.LayerNorm.weight', 'explanation.encoder.layer.7.attention.output.LayerNorm.bias', 'explanation.encoder.layer.7.crossattention.self.query.weight', 'explanation.encoder.layer.7.crossattention.self.query.bias', 'explanation.encoder.layer.7.crossattention.self.key.weight', 'explanation.encoder.layer.7.crossattention.self.key.bias', 'explanation.encoder.layer.7.crossattention.self.value.weight', 'explanation.encoder.layer.7.crossattention.self.value.bias', 'explanation.encoder.layer.7.crossattention.output.dense.weight', 'explanation.encoder.layer.7.crossattention.output.dense.bias', 'explanation.encoder.layer.7.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.7.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.7.intermediate.dense.weight', 'explanation.encoder.layer.7.intermediate.dense.bias', 'explanation.encoder.layer.7.output.dense.weight', 'explanation.encoder.layer.7.output.dense.bias', 'explanation.encoder.layer.7.output.LayerNorm.weight', 'explanation.encoder.layer.7.output.LayerNorm.bias', 'explanation.encoder.layer.8.attention.self.query.weight', 'explanation.encoder.layer.8.attention.self.query.bias', 'explanation.encoder.layer.8.attention.self.key.weight', 'explanation.encoder.layer.8.attention.self.key.bias', 'explanation.encoder.layer.8.attention.self.value.weight', 'explanation.encoder.layer.8.attention.self.value.bias', 'explanation.encoder.layer.8.attention.output.dense.weight', 'explanation.encoder.layer.8.attention.output.dense.bias', 'explanation.encoder.layer.8.attention.output.LayerNorm.weight', 'explanation.encoder.layer.8.attention.output.LayerNorm.bias', 'explanation.encoder.layer.8.crossattention.self.query.weight', 'explanation.encoder.layer.8.crossattention.self.query.bias', 'explanation.encoder.layer.8.crossattention.self.key.weight', 'explanation.encoder.layer.8.crossattention.self.key.bias', 'explanation.encoder.layer.8.crossattention.self.value.weight', 'explanation.encoder.layer.8.crossattention.self.value.bias', 'explanation.encoder.layer.8.crossattention.output.dense.weight', 'explanation.encoder.layer.8.crossattention.output.dense.bias', 'explanation.encoder.layer.8.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.8.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.8.intermediate.dense.weight', 'explanation.encoder.layer.8.intermediate.dense.bias', 'explanation.encoder.layer.8.output.dense.weight', 'explanation.encoder.layer.8.output.dense.bias', 'explanation.encoder.layer.8.output.LayerNorm.weight', 'explanation.encoder.layer.8.output.LayerNorm.bias', 'explanation.encoder.layer.9.attention.self.query.weight', 'explanation.encoder.layer.9.attention.self.query.bias', 'explanation.encoder.layer.9.attention.self.key.weight', 'explanation.encoder.layer.9.attention.self.key.bias', 'explanation.encoder.layer.9.attention.self.value.weight', 'explanation.encoder.layer.9.attention.self.value.bias', 'explanation.encoder.layer.9.attention.output.dense.weight', 'explanation.encoder.layer.9.attention.output.dense.bias', 'explanation.encoder.layer.9.attention.output.LayerNorm.weight', 'explanation.encoder.layer.9.attention.output.LayerNorm.bias', 'explanation.encoder.layer.9.crossattention.self.query.weight', 'explanation.encoder.layer.9.crossattention.self.query.bias', 'explanation.encoder.layer.9.crossattention.self.key.weight', 'explanation.encoder.layer.9.crossattention.self.key.bias', 'explanation.encoder.layer.9.crossattention.self.value.weight', 'explanation.encoder.layer.9.crossattention.self.value.bias', 'explanation.encoder.layer.9.crossattention.output.dense.weight', 'explanation.encoder.layer.9.crossattention.output.dense.bias', 'explanation.encoder.layer.9.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.9.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.9.intermediate.dense.weight', 'explanation.encoder.layer.9.intermediate.dense.bias', 'explanation.encoder.layer.9.output.dense.weight', 'explanation.encoder.layer.9.output.dense.bias', 'explanation.encoder.layer.9.output.LayerNorm.weight', 'explanation.encoder.layer.9.output.LayerNorm.bias', 'explanation.encoder.layer.10.attention.self.query.weight', 'explanation.encoder.layer.10.attention.self.query.bias', 'explanation.encoder.layer.10.attention.self.key.weight', 'explanation.encoder.layer.10.attention.self.key.bias', 'explanation.encoder.layer.10.attention.self.value.weight', 'explanation.encoder.layer.10.attention.self.value.bias', 'explanation.encoder.layer.10.attention.output.dense.weight', 'explanation.encoder.layer.10.attention.output.dense.bias', 'explanation.encoder.layer.10.attention.output.LayerNorm.weight', 'explanation.encoder.layer.10.attention.output.LayerNorm.bias', 'explanation.encoder.layer.10.crossattention.self.query.weight', 'explanation.encoder.layer.10.crossattention.self.query.bias', 'explanation.encoder.layer.10.crossattention.self.key.weight', 'explanation.encoder.layer.10.crossattention.self.key.bias', 'explanation.encoder.layer.10.crossattention.self.value.weight', 'explanation.encoder.layer.10.crossattention.self.value.bias', 'explanation.encoder.layer.10.crossattention.output.dense.weight', 'explanation.encoder.layer.10.crossattention.output.dense.bias', 'explanation.encoder.layer.10.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.10.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.10.intermediate.dense.weight', 'explanation.encoder.layer.10.intermediate.dense.bias', 'explanation.encoder.layer.10.output.dense.weight', 'explanation.encoder.layer.10.output.dense.bias', 'explanation.encoder.layer.10.output.LayerNorm.weight', 'explanation.encoder.layer.10.output.LayerNorm.bias', 'explanation.encoder.layer.11.attention.self.query.weight', 'explanation.encoder.layer.11.attention.self.query.bias', 'explanation.encoder.layer.11.attention.self.key.weight', 'explanation.encoder.layer.11.attention.self.key.bias', 'explanation.encoder.layer.11.attention.self.value.weight', 'explanation.encoder.layer.11.attention.self.value.bias', 'explanation.encoder.layer.11.attention.output.dense.weight', 'explanation.encoder.layer.11.attention.output.dense.bias', 'explanation.encoder.layer.11.attention.output.LayerNorm.weight', 'explanation.encoder.layer.11.attention.output.LayerNorm.bias', 'explanation.encoder.layer.11.crossattention.self.query.weight', 'explanation.encoder.layer.11.crossattention.self.query.bias', 'explanation.encoder.layer.11.crossattention.self.key.weight', 'explanation.encoder.layer.11.crossattention.self.key.bias', 'explanation.encoder.layer.11.crossattention.self.value.weight', 'explanation.encoder.layer.11.crossattention.self.value.bias', 'explanation.encoder.layer.11.crossattention.output.dense.weight', 'explanation.encoder.layer.11.crossattention.output.dense.bias', 'explanation.encoder.layer.11.crossattention.output.LayerNorm.weight', 'explanation.encoder.layer.11.crossattention.output.LayerNorm.bias', 'explanation.encoder.layer.11.intermediate.dense.weight', 'explanation.encoder.layer.11.intermediate.dense.bias', 'explanation.encoder.layer.11.output.dense.weight', 'explanation.encoder.layer.11.output.dense.bias', 'explanation.encoder.layer.11.output.LayerNorm.weight', 'explanation.encoder.layer.11.output.LayerNorm.bias', 'explanation_pghead.encoder_attention.linear_q.weight', 'explanation_pghead.encoder_attention.linear_q.bias', 'explanation_pghead.encoder_attention.linear_k.weight', 'explanation_pghead.encoder_attention.linear_k.bias', 'explanation_pghead.generation_dist.weight', 'explanation_pghead.generation_dist.bias', 'explanation_pghead.generation_prob_linear.weight', 'explanation_pghead.generation_prob_linear.bias']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in state_dict.keys() if \"explanation\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 3072])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['explanation.encoder.layer.11.output.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['explanation.encoder.layer.11.attention.output.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['explanation.encoder.layer.11.crossattention.output.dense.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict['explanation.embeddings.LayerNorm.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = data_path / 'dataset' / 'pen.json'\n",
    "test_data = Dataset(dataset_file, number_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.select_items_with_file(data_path / 'experiments'/ 'pen' /'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W Context.cpp:70] Warning: torch.use_deterministic_algorithms is in beta, and its design and functionality may change in the future. (function operator())\n"
     ]
    }
   ],
   "source": [
    "set_seed(config['seed'])\n",
    "batch = test_data.get_minibatches(batch_size=1, for_testing=True)\n",
    "output = nvix(\n",
    "    text=batch[0].text.to(nvix.device),\n",
    "    beam=config['beam_for_equation'], \n",
    "    beam_expl=config['beam_for_explanation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3330e-01, 5.4100e-06, 1.4479e-06, 3.2128e-06, 1.9734e-07, 2.7834e-06,\n",
       "         4.6896e-08, 3.4509e-07, 1.9025e-06, 2.1453e-08, 1.1091e-06, 4.9619e-05,\n",
       "         7.8588e-06, 2.3247e-05, 4.8032e-06, 1.2212e-09, 3.3330e-01, 3.3330e-01]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvix.explanation_pghead.attention_score[0] # Attention score: [B, T, S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nvix.explanation_pghead.attention_score[0].shape # Attention score: [B, T, S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].text.as_dict()['tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded, num_out = nvix.encoder(batch[0].text)\n",
    "encoded.as_dict()['vector'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': <common.data.text.Text at 0x7f9c628b3c10>,\n",
       " 'equation': Equation(operator=Label([[0, 1, 6, 3, 2]]), operands=[Label([[-1, -1, 24, 23, -1]]), Label([[-1, -1, 48, 49, -1]])]),\n",
       " 'explanation': [Explanation(numbers=$[Label([[102, 1996, 2193, 1997, 7338, 6158, 2011, 7441, 102], [102, 1996, 3177, 1997, 7441, 102, -1, -1, -1]]), Label([[102, 1996, 3292, 1997, 7441, 1005, 1055, 27244, 2075, 1999, 7338, 102, -1, -1], [102, 1996, 3177, 1997, 7441, 1005, 1055, 27244, 2075, 1999, 7338, 2566, 3178, 102]]), Label([[102, 1996, 2561, 2193, 1997, 7338, 20354, 102, -1], [102, 1996, 2193, 1997, 7338, 20354, 2566, 3178, 102]])], variables=$[Label([[102, 1996, 2193, 1997, 2847, 6158, 2011, 7441, 102]]), Label([[102, 1996, 3091, 1997, 7441, 1005, 1055, 27244, 2075, 2051, 102]]), Label([[102, 1996, 2193, 1997, 2847, 20354, 102]])], worker=$-1)],\n",
       " 'info': [<common.data.ExtraInfo at 0x7f9c42fa57c0>]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0].as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eqn_ignore': {1},\n",
       " 'var_expl': [Label([[102, 1996, 3091, 1997, 2051, 1997, 27244, 2075, 102, -1, -1, -1, -1]])],\n",
       " 'num_expl': [Label([[102, 1996, 3292, 1997, 27244, 2075, 1999, 7338, 102, -1, -1], [102, 1996, 3177, 1997, 27244, 2075, 1999, 7338, 2566, 3178, 102]])],\n",
       " 'explanation': [Explanation(numbers=$[Label([[102, 1996, 3292, 1997, 27244, 2075, 1999, 7338, 102, -1, -1], [102, 1996, 3177, 1997, 27244, 2075, 1999, 7338, 2566, 3178, 102]])], variables=$[Label([[102, 1996, 3091, 1997, 2051, 1997, 27244, 2075, 102, -1, -1, -1, -1]])], worker=$0)],\n",
       " 'equation': Equation(operator=Label([[0]]), operands=[Label([[-1]]), Label([[-1]])])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 11])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['num_expl'][0].as_dict()['indices'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# http://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor\n",
    "def plot_head_map(mma, target_labels, source_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(mma, cmap=plt.cm.Blues)\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(numpy.arange(mma.shape[1]) + 0.5, minor=False) # mma.shape[1] = target seq 길이\n",
    "    ax.set_yticks(numpy.arange(mma.shape[0]) + 0.5, minor=False) # mma.shape[0] = input seq 길이\n",
    " \n",
    "    # without this I get some extra columns rows\n",
    "    # http://stackoverflow.com/questions/31601351/why-does-this-matplotlib-heatmap-have-an-extra-blank-column\n",
    "    ax.set_xlim(0, int(mma.shape[1]))\n",
    "    ax.set_ylim(0, int(mma.shape[0]))\n",
    " \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    " \n",
    "    # source words -> column labels\n",
    "    ax.set_xticklabels(source_labels, minor=False)\n",
    "    # target words -> row labels\n",
    "    ax.set_yticklabels(target_labels, minor=False)\n",
    " \n",
    "    plt.xticks(rotation=45)\n",
    " \n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def read_plot_alignment_matrices(source_labels, target_labels, alpha):\n",
    " \n",
    "    mma = alpha.cpu().data.numpy()\n",
    " \n",
    "    plot_head_map(mma, target_labels, source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_753016/4086457787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#read_plot_alignment_matrices(input_str, output_str, attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhead_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/bertviz/head_view.py\u001b[0m in \u001b[0;36mhead_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, layer, heads, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minclude_layers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0minclude_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentence_b_start\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             attn_data.append(\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/bertviz/util.py\u001b[0m in \u001b[0;36mformat_attention\u001b[0;34m(attention, layers, heads)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 1 x num_heads x seq_len x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             raise ValueError(\"The attention tensor does not have the correct number of dimensions. Make sure you set \"\n\u001b[0m\u001b[1;32m     12\u001b[0m                              \"output_attentions=True when initializing your model.\")\n\u001b[1;32m     13\u001b[0m         \u001b[0mlayer_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "attn = state_dict['explanation.encoder.layer.11.output.LayerNorm.weight']\n",
    "attn.shape\n",
    "head_view(attn, encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"the sears tower in chicago is 145 ##0 feet tall . the john hancock center in chicago is 112 ##7 feet tall . suppose you are asked to build a small - scale replica of each . if you make the sears tower 3 meter tall , what would be the approximate height of the john hancock replica ? round your answer to the nearest hundred ##th .\"\n",
    "encoded_input = tokenizer.encode(input_str)\n",
    "len(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_str = \"the height of the sears tower\"\n",
    "encoded_output = tokenizer.encode(output_str)\n",
    "len(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 1,\n",
       " 'batch_size': 16,\n",
       " 'beam_for_equation': 3,\n",
       " 'beam_for_explanation': 5,\n",
       " 'dataset': '/home/bydelta/SimpleFESTA/resource/dataset/pen.json',\n",
       " 'learner': {'model': 'SWAN_P1',\n",
       "  'encoder': 'google/electra-base-discriminator',\n",
       "  'equation': {'hidden_dim': 0, 'intermediate_dim': 0, 'layer': 6, 'head': 0},\n",
       "  'explanation': {'encoder': 'google/electra-base-discriminator',\n",
       "   'shuffle': True}},\n",
       " 'resource': {'num_gpus': 1.0, 'num_cpus': 1.0},\n",
       " 'experiment': {'dev': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/dev',\n",
       "   'period': 100},\n",
       "  'train': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/train'},\n",
       "  'test': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/test',\n",
       "   'period': 500}},\n",
       " 'grad_clip': 10.0,\n",
       " 'optimizer': {'type': 'lamb',\n",
       "  'lr': 0.00176,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'debias': True},\n",
       " 'scheduler': {'type': 'warmup-linear',\n",
       "  'num_warmup_epochs': 10.0,\n",
       "  'num_total_epochs': 500},\n",
       " 'window': 3}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = data_path / 'dataset' / 'pen.json'\n",
    "\n",
    "KEY_DATASET = 'dataset'\n",
    "KEY_SEED = 'seed'\n",
    "\n",
    "exp_base = {\n",
    "        KEY_DATASET: str(dataset_file.absolute()),\n",
    "        KEY_SEED: config['seed']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check, test_results = run_model_for_attention(chpt_path, **exp_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
