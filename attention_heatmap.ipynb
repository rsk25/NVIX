{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from bertviz import head_view\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.swan import SWANPhase1Only\n",
    "from model.base import chkpt\n",
    "from test_model import load_config, run_model_for_attention\n",
    "from common.dataset import Dataset\n",
    "from learner import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = Path('.')\n",
    "data_path = main_path / 'resource'\n",
    "chpt_path = main_path / 'runs_copy' / 'best_SWAN_P1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torch.load(chpt_path / 'tokenizer.pt')\n",
    "checkpoint = torch.load(chpt_path / 'SWANPhase1Only.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(chpt_path)\n",
    "nvix = SWANPhase1Only.create_or_load(path=str(chpt_path), **config)\n",
    "nvix.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = nvix.state_dict()\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = data_path / 'dataset' / 'pen.json'\n",
    "test_data = Dataset(dataset_file, number_window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.select_items_with_file(data_path / 'experiments'/ 'pen' /'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "365"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(config['seed'])\n",
    "batch = test_data.get_minibatches(batch_size=1, for_testing=True)\n",
    "output = nvix(\n",
    "    text=batch[0].text.to(nvix.device),\n",
    "    beam=config['beam_for_equation'], \n",
    "    beam_expl=config['beam_for_explanation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eqn_ignore': {1},\n",
       " 'var_expl': [Label([[102, 1996, 3091, 1997, 2051, 1997, 27244, 2075, 102, -1, -1, -1, -1]])],\n",
       " 'num_expl': [Label([[102, 1996, 3292, 1997, 27244, 2075, 1999, 7338, 102, -1, -1], [102, 1996, 3177, 1997, 27244, 2075, 1999, 7338, 2566, 3178, 102]])],\n",
       " 'explanation': [Explanation(numbers=$[Label([[102, 1996, 3292, 1997, 27244, 2075, 1999, 7338, 102, -1, -1], [102, 1996, 3177, 1997, 27244, 2075, 1999, 7338, 2566, 3178, 102]])], variables=$[Label([[102, 1996, 3091, 1997, 2051, 1997, 27244, 2075, 102, -1, -1, -1, -1]])], worker=$0)],\n",
       " 'equation': Equation(operator=Label([[0]]), operands=[Label([[-1]]), Label([[-1]])])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# http://stackoverflow.com/questions/14391959/heatmap-in-matplotlib-with-pcolor\n",
    "def plot_head_map(mma, target_labels, source_labels):\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.pcolor(mma, cmap=plt.cm.Blues)\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(numpy.arange(mma.shape[1]) + 0.5, minor=False) # mma.shape[1] = target seq 길이\n",
    "    ax.set_yticks(numpy.arange(mma.shape[0]) + 0.5, minor=False) # mma.shape[0] = input seq 길이\n",
    " \n",
    "    # without this I get some extra columns rows\n",
    "    # http://stackoverflow.com/questions/31601351/why-does-this-matplotlib-heatmap-have-an-extra-blank-column\n",
    "    ax.set_xlim(0, int(mma.shape[1]))\n",
    "    ax.set_ylim(0, int(mma.shape[0]))\n",
    " \n",
    "    # want a more natural, table-like display\n",
    "    ax.invert_yaxis()\n",
    "    ax.xaxis.tick_top()\n",
    " \n",
    "    # source words -> column labels\n",
    "    ax.set_xticklabels(source_labels, minor=False)\n",
    "    # target words -> row labels\n",
    "    ax.set_yticklabels(target_labels, minor=False)\n",
    " \n",
    "    plt.xticks(rotation=45)\n",
    " \n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def read_plot_alignment_matrices(source_labels, target_labels, alpha):\n",
    " \n",
    "    mma = alpha.cpu().data.numpy()\n",
    " \n",
    "    plot_head_map(mma, target_labels, source_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_753016/4086457787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#read_plot_alignment_matrices(input_str, output_str, attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhead_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/bertviz/head_view.py\u001b[0m in \u001b[0;36mhead_view\u001b[0;34m(attention, tokens, sentence_b_start, prettify_tokens, layer, heads, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minclude_layers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0minclude_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msentence_b_start\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             attn_data.append(\n",
      "\u001b[0;32m~/venv/lib/python3.8/site-packages/bertviz/util.py\u001b[0m in \u001b[0;36mformat_attention\u001b[0;34m(attention, layers, heads)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# 1 x num_heads x seq_len x seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             raise ValueError(\"The attention tensor does not have the correct number of dimensions. Make sure you set \"\n\u001b[0m\u001b[1;32m     12\u001b[0m                              \"output_attentions=True when initializing your model.\")\n\u001b[1;32m     13\u001b[0m         \u001b[0mlayer_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "attn = state_dict['explanation.encoder.layer.11.output.LayerNorm.weight']\n",
    "attn.shape\n",
    "head_view(attn, encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"the sears tower in chicago is 145 ##0 feet tall . the john hancock center in chicago is 112 ##7 feet tall . suppose you are asked to build a small - scale replica of each . if you make the sears tower 3 meter tall , what would be the approximate height of the john hancock replica ? round your answer to the nearest hundred ##th .\"\n",
    "encoded_input = tokenizer.encode(input_str)\n",
    "len(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_str = \"the height of the sears tower\"\n",
    "encoded_output = tokenizer.encode(output_str)\n",
    "len(encoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 1,\n",
       " 'batch_size': 16,\n",
       " 'beam_for_equation': 3,\n",
       " 'beam_for_explanation': 5,\n",
       " 'dataset': '/home/bydelta/SimpleFESTA/resource/dataset/pen.json',\n",
       " 'learner': {'model': 'SWAN_P1',\n",
       "  'encoder': 'google/electra-base-discriminator',\n",
       "  'equation': {'hidden_dim': 0, 'intermediate_dim': 0, 'layer': 6, 'head': 0},\n",
       "  'explanation': {'encoder': 'google/electra-base-discriminator',\n",
       "   'shuffle': True}},\n",
       " 'resource': {'num_gpus': 1.0, 'num_cpus': 1.0},\n",
       " 'experiment': {'dev': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/dev',\n",
       "   'period': 100},\n",
       "  'train': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/train'},\n",
       "  'test': {'split_file': '/home/bydelta/SimpleFESTA/resource/experiments/pen/test',\n",
       "   'period': 500}},\n",
       " 'grad_clip': 10.0,\n",
       " 'optimizer': {'type': 'lamb',\n",
       "  'lr': 0.00176,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'debias': True},\n",
       " 'scheduler': {'type': 'warmup-linear',\n",
       "  'num_warmup_epochs': 10.0,\n",
       "  'num_total_epochs': 500},\n",
       " 'window': 3}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file = data_path / 'dataset' / 'pen.json'\n",
    "\n",
    "KEY_DATASET = 'dataset'\n",
    "KEY_SEED = 'seed'\n",
    "\n",
    "exp_base = {\n",
    "        KEY_DATASET: str(dataset_file.absolute()),\n",
    "        KEY_SEED: config['seed']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_check, test_results = run_model_for_attention(chpt_path, **exp_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
